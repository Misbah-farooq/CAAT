{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eee0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.pipelines import HUBERT_BASE\n",
    "from pytorch_metric_learning import losses, miners\n",
    "import os\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Models\n",
    "class RobertaClassifierWithEmbeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.classifier = nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "        logits = self.classifier(cls_embedding)\n",
    "        return logits, cls_embedding\n",
    "\n",
    "class HuBERTEmotionDataset(Dataset):\n",
    "    def __init__(self, wav_paths, text_data, labels, tokenizer, target_length=48000):\n",
    "        self.wav_paths = wav_paths\n",
    "        self.text_data = text_data\n",
    "        self.labels = torch.tensor(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=48000)\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sr = torchaudio.load(self.wav_paths[idx])\n",
    "        if sr != 48000:\n",
    "            waveform = self.resampler(waveform)\n",
    "\n",
    "        if waveform.size(1) < self.target_length:\n",
    "            padding = self.target_length - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        elif waveform.size(1) > self.target_length:\n",
    "            waveform = waveform[:, :self.target_length]\n",
    "\n",
    "        text = self.text_data[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "        return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0), waveform.squeeze(0), self.labels[idx]\n",
    "\n",
    "class HuBERTClassifierWithEmbeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hubert = HUBERT_BASE.get_model()\n",
    "        self.project = nn.Linear(768, 768)\n",
    "        self.classifier = nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "        features, _ = self.hubert(waveforms)\n",
    "        embeddings = features.mean(dim=1)\n",
    "        embeddings = self.project(embeddings)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits, embeddings\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attention_text_to_speech = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=4, batch_first=True)\n",
    "        self.attention_speech_to_text = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=4, batch_first=True)\n",
    "        self.fc = nn.Linear(embed_dim * 2, 4)\n",
    "\n",
    "    def forward(self, text_embeddings, speech_embeddings):\n",
    "        text_embed_seq = text_embeddings.unsqueeze(1)\n",
    "        speech_embed_seq = speech_embeddings.unsqueeze(1)\n",
    "\n",
    "        attended_text_to_speech, attn_weights_t2s = self.attention_text_to_speech(\n",
    "            query=text_embed_seq, key=speech_embed_seq, value=speech_embed_seq, need_weights=True\n",
    "        )\n",
    "        attended_speech_to_text, attn_weights_s2t = self.attention_speech_to_text(\n",
    "            query=speech_embed_seq, key=text_embed_seq, value=text_embed_seq, need_weights=True\n",
    "        )\n",
    "\n",
    "        combined = torch.cat([\n",
    "            attended_text_to_speech.squeeze(1),\n",
    "            attended_speech_to_text.squeeze(1)\n",
    "        ], dim=-1)\n",
    "\n",
    "        logits = self.fc(combined)\n",
    "\n",
    "        attention_scores = ((attn_weights_t2s.squeeze(1) + attn_weights_s2t.squeeze(1)) / 2).squeeze(1)\n",
    "        return logits, attention_scores\n",
    "\n",
    "class CrossModalEmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_model = RobertaClassifierWithEmbeddings()\n",
    "        self.speech_model = HuBERTClassifierWithEmbeddings()\n",
    "        self.cross_modal_attention = CrossModalAttention(embed_dim=768)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, waveforms):\n",
    "        text_logits, text_embeddings = self.text_model(input_ids, attention_mask)\n",
    "        speech_logits, speech_embeddings = self.speech_model(waveforms)\n",
    "        final_logits, attention_scores = self.cross_modal_attention(text_embeddings, speech_embeddings)\n",
    "        return final_logits, text_embeddings, speech_embeddings, attention_scores\n",
    "\n",
    "# Losses\n",
    "triplet_loss_fn = losses.TripletMarginLoss(margin=0.2)\n",
    "miner = miners.TripletMarginMiner(margin=0.2, type_of_triplets=\"hard\")\n",
    "\n",
    "# Training and Evaluation\n",
    "def train_model(model, train_loader, optimizer, ce_criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for text_input_ids, text_attention_mask, waveforms, labels in train_loader:\n",
    "        text_input_ids, text_attention_mask, waveforms, labels = (\n",
    "            text_input_ids.to(device),\n",
    "            text_attention_mask.to(device),\n",
    "            waveforms.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        logits, text_embeddings, speech_embeddings, attention_scores = model(text_input_ids, text_attention_mask, waveforms)\n",
    "        ce_loss = ce_criterion(logits, labels)\n",
    "\n",
    "        weights = torch.softmax(attention_scores, dim=0).unsqueeze(1)\n",
    "        combined_embeddings = weights * torch.cat([text_embeddings, speech_embeddings], dim=1)\n",
    "\n",
    "        hard_triplets = miner(combined_embeddings, labels)\n",
    "        triplet_loss = triplet_loss_fn(combined_embeddings, labels, hard_triplets) if len(hard_triplets[0]) > 0 else torch.tensor(0.0).to(device)\n",
    "        loss = ce_loss \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for text_input_ids, text_attention_mask, waveforms, labels in val_loader:\n",
    "            text_input_ids, text_attention_mask, waveforms, labels = (\n",
    "                text_input_ids.to(device),\n",
    "                text_attention_mask.to(device),\n",
    "                waveforms.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            logits, _, _, _ = model(text_input_ids, text_attention_mask, waveforms)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    return accuracy, precision, recall, f1, all_labels, all_preds\n",
    "\n",
    "# Load Data\n",
    "full_data = pd.read_csv('./iemocap.csv')\n",
    "emotion_mapping = {'neu': 0, 'hap': 1, 'sad': 2, 'ang': 3}\n",
    "full_data['Emotion_label'] = full_data['Emotion'].map(emotion_mapping)\n",
    "wav_paths = np.array(full_data['wav_path'].tolist())\n",
    "labels = np.array(full_data['Emotion_label'].tolist())\n",
    "text_data = np.array(full_data['Utterance'].tolist())\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "fold_results = []\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "best_val_loader = None\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(wav_paths, labels)):\n",
    "    print(f\"\\n----- Fold {fold + 1} -----\")\n",
    "    train_dataset = HuBERTEmotionDataset(wav_paths[train_idx], text_data[train_idx], labels[train_idx], tokenizer)\n",
    "    val_dataset = HuBERTEmotionDataset(wav_paths[val_idx], text_data[val_idx], labels[val_idx], tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = CrossModalEmotionClassifier().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.99))\n",
    "    ce_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        train_loss = train_model(model, train_loader, optimizer, ce_criterion)\n",
    "        accuracy, precision, recall, f1, _, _ = evaluate_model(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "        best_val_loader = val_loader\n",
    "\n",
    "# After all folds: evaluate best model\n",
    "print(\"\\n===== Best Model Evaluation =====\")\n",
    "_, _, _, _, all_labels, all_preds = evaluate_model(best_model, best_val_loader)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"neu\", \"hap\", \"sad\", \"ang\"])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Best Model\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Confusion Matrix with Percentages\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Normalize to percentage\n",
    "\n",
    "# Create a confusion matrix display\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percentage, display_labels=[\"neu\", \"hap\", \"sad\", \"ang\"])\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp.plot(cmap='Blues', values_format=\".2f\")  # '.2f' for 2 decimal places\n",
    "plt.title(\"Confusion Matrix - Best Model (Percentage)\")\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Assuming you have all_labels and all_preds\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Normalize the confusion matrix to percentage\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Create a confusion matrix display\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percentage, display_labels=[\"neu\", \"hap\", \"sad\", \"ang\"])\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp.plot(cmap='Blues', values_format=\".2f\")  # '.2f' for 2 decimal places\n",
    "plt.title(\" Cross-modal Attention Weighted Triplet loss (CAAT)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have all_labels and all_preds\n",
    "report = classification_report(all_labels, all_preds, target_names=[\"neu\", \"hap\", \"sad\", \"ang\"],digits=4)\n",
    "\n",
    "print(report)\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract embeddings from the best model\n",
    "def extract_embeddings(model, data_loader):\n",
    "    model.eval()\n",
    "    all_text_embeddings = []\n",
    "    all_speech_embeddings = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for text_input_ids, text_attention_mask, waveforms, labels in data_loader:\n",
    "            text_input_ids, text_attention_mask, waveforms, labels = (\n",
    "                text_input_ids.to(device),\n",
    "                text_attention_mask.to(device),\n",
    "                waveforms.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            _, text_embeddings, speech_embeddings, _ = model(text_input_ids, text_attention_mask, waveforms)\n",
    "            all_text_embeddings.append(text_embeddings.cpu().numpy())\n",
    "            all_speech_embeddings.append(speech_embeddings.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    text_embeddings = np.concatenate(all_text_embeddings, axis=0)\n",
    "    speech_embeddings = np.concatenate(all_speech_embeddings, axis=0)\n",
    "    return text_embeddings, speech_embeddings, np.array(all_labels)\n",
    "\n",
    "# Combine both text and speech embeddings\n",
    "text_embeddings, speech_embeddings, labels = extract_embeddings(best_model, best_val_loader)\n",
    "combined_embeddings = np.concatenate([text_embeddings, speech_embeddings], axis=1)\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_embeddings = tsne.fit_transform(combined_embeddings)\n",
    "\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "tsne_df = pd.DataFrame(tsne_embeddings, columns=[\"Dimension 1\", \"Dimension 2\"])\n",
    "#tsne_df['Emotion'] = labels\n",
    "emotion_mapping = {0: 'neu', 1: 'hap', 2: 'sad', 3: 'ang'}\n",
    "tsne_df['Emotion'] = [emotion_mapping[label] for label in labels]\n",
    "\n",
    "# Plot t-SNE\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=\"Dimension 1\", y=\"Dimension 2\", hue=\"Emotion\", palette=\"bright\", data=tsne_df, s=60, legend=\"full\")\n",
    "plt.title(\"t-SNE-Cross-modal Attention Weighted Triplet loss (CAAT)\", fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Extract embeddings from the best model\n",
    "def extract_embeddings(model, data_loader):\n",
    "    model.eval()\n",
    "    all_text_embeddings = []\n",
    "    all_speech_embeddings = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for text_input_ids, text_attention_mask, waveforms, labels in data_loader:\n",
    "            text_input_ids, text_attention_mask, waveforms, labels = (\n",
    "                text_input_ids.to(device),\n",
    "                text_attention_mask.to(device),\n",
    "                waveforms.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            _, text_embeddings, speech_embeddings, _ = model(text_input_ids, text_attention_mask, waveforms)\n",
    "            all_text_embeddings.append(text_embeddings.cpu().numpy())\n",
    "            all_speech_embeddings.append(speech_embeddings.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    text_embeddings = np.concatenate(all_text_embeddings, axis=0)\n",
    "    speech_embeddings = np.concatenate(all_speech_embeddings, axis=0)\n",
    "    return text_embeddings, speech_embeddings, np.array(all_labels)\n",
    "\n",
    "# Combine both text and speech embeddings\n",
    "text_embeddings, speech_embeddings, labels = extract_embeddings(best_model, best_val_loader)\n",
    "combined_embeddings = np.concatenate([text_embeddings, speech_embeddings], axis=1)\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "tsne_embeddings = tsne.fit_transform(combined_embeddings)\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "tsne_df = pd.DataFrame(tsne_embeddings, columns=[\"Dimension 1\", \"Dimension 2\"])\n",
    "emotion_mapping = {0: 'neu', 1: 'hap', 2: 'sad', 3: 'ang'}\n",
    "tsne_df['Emotion'] = [emotion_mapping[label] for label in labels]\n",
    "\n",
    "# Define custom palette with orange for 'hap' (happy)\n",
    "custom_palette = {'neu': 'blue', 'hap': 'orange', 'sad': 'lightgreen', 'ang': 'red'}\n",
    "\n",
    "# Plot t-SNE\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=\"Dimension 1\", y=\"Dimension 2\", hue=\"Emotion\", palette=custom_palette, data=tsne_df, s=60, legend=\"full\")\n",
    "plt.title(\"t-SNE - Cross-modal attention weighted triplet-loss(CAAT)\", fontsize=12)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
